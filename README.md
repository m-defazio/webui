# ğŸ§  Open WebUI + Ollama + Nginx

Un'infrastruttura completa per utilizzare [Open WebUI](https://github.com/open-webui/open-webui) con modelli LLM locali tramite [Ollama](https://ollama.com), con supporto per OpenAI API e Nginx come reverse proxy.

> âœ… Il container Ã¨ preconfigurato per scaricare automaticamente il modello `deepseek-r1:1.5b`.

---

## ğŸš€ Avvio rapido

### Clona il repository:

```bash
git clone https://github.com/m-defazio/webui.git
cd webui
```

### Crea un file `.env` con la tua API Key di OpenAI (facoltativa):

```bash
echo "OPENAI_KEY=sk-xxxxx..." > .env
```

### Avvia l'infrastruttura:

```bash
docker compose up -d
```

### Accedi allâ€™interfaccia Web:

Apri il browser e vai su ğŸ‘‰ [http://localhost](http://localhost)

---

## ğŸ—‚ Struttura del progetto

```bash
.
â”œâ”€â”€ docker-compose.yml      # Gestione dei container
â”œâ”€â”€ Dockerfile              # Estende l'immagine Ollama con entrypoint personalizzato
â”œâ”€â”€ entrypoint.sh           # Script per avvio Ollama e download modelli
â”œâ”€â”€ nginx/
â”‚   â”œâ”€â”€ default.conf        # Configurazione Nginx (personalizzabile)
â”‚   â””â”€â”€ certs/              # Certificati TLS (opzionali)
â””â”€â”€ .env                    # File con la variabile OPENAI_KEY (non incluso nel repo)
```
---

## ğŸ§  Modello incluso

Il sistema Ã¨ preconfigurato per scaricare automaticamente:

- `deepseek-r1:1.5b` â€“ modello leggero e ottimizzato per l'esecuzione locale

Puoi modificare il modello cambiando la riga nel file `entrypoint.sh`.

---

## ğŸ”§ Personalizzazioni

### âœï¸ Cambiare il modello LLM

Nel file `entrypoint.sh`, modifica questa riga:

```sh
MODELS="deepseek-r1:1.5b"
```
Sostituiscila con qualsiasi modello disponibile su [https://ollama.com/library](https://ollama.com/library), ad esempio:

```sh
MODELS="llama2:7b"
```
Poi ricostruisci e riavvia:

```bash
docker compose build ollama
docker compose up -d
```
---

## ğŸŒ Accesso ai servizi

| Servizio     | Porta | URL                    |
|--------------|-------|------------------------|
| Open WebUI   | 443   | https://localhost      |
| Ollama API   | 11434 | http://localhost:11434 |

---

## ğŸ” HTTPS (opzionale)

Se vuoi abilitare HTTPS:

1. Aggiungi i certificati `.crt` e `.key` nella cartella `nginx/certs`
2. Modifica il file `nginx/default.conf` per gestire le richieste HTTPS

---

## ğŸ›‘ Stop & Pulizia

### Ferma tutti i container:

```bash
docker compose down
```
### Elimina anche volumi e immagini:

```bash
docker compose down -v --rmi all
```
---

## ğŸ“ƒ Licenza

Distribuito sotto licenza **MIT**.

---

## ğŸ™Œ Crediti

- [Open WebUI](https://github.com/open-webui/open-webui)
- [Ollama](https://ollama.com)
- [DeepSeek](https://huggingface.co/deepseek-ai)
